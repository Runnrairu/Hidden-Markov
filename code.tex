\documentclass{jsarticle}
\usepackage[dvipdfmx]{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{type1cm}
\usepackage{bm}
\title{機械学習入門}
\author{。}
\begin{document}
\maketitle
\section{確率論の基礎の復習}
この章では、機械学習の数学的定式化に必要な確率論の基礎中の基礎を復習する。\\
　目的はあくまで機械学習への応用であるため、細々とした議論は行わず、機械学習を定式化するために必要な最低限の項目だけ解説する\\
　\\
 　\\
\scalebox{1.1}{1.確率空間と確率変数}\\
　標本空間$\Omega$に対し、次を満たす部分集合族$\mathcal{F}$をσ加法族と呼ぶ
\begin{eqnarray}
&1.&\phi,\Omega\in\mathcal{F}\\
&2.&A\in\mathcal{F}\rightarrow A^c\in\mathcal{F}\\
&3.&可算個の\Omega の部分集合 A_1,A_2,......,があり、任意のnに対してA_n\in\mathcal{F}なら、\cup^\infty_{n=1} A_n\in\mathcal{F}
\end{eqnarray}
　写像$P:\mathcal{F}\rightarrow[0,1]$が次を満たすとき、$P$を確率測度という
\begin{eqnarray}
&1.&P(\Omega)=1\\
&2.&互いに素な可算個の集合A_1,A_2,...,\in\mathcal{F}に対し、P(\cup^\infty_{n=1}A_n)=\Sigma^\infty_{n=1}P(A_n)
\end{eqnarray}
　この$(\Omega,\mathcal{F},P)$の組を、確率空間と呼ぶ\\
　$\mathcal{B}(\mathbb{R})$で、$\mathbb{R}$の開集合をすべて含む最小のσ加法族を表し、ボレル集合族と呼ぶ。\\
　写像$X:\Omega\rightarrow\mathbb{R}$が次の条件を満たすとき、（1次元実）確率変数であるという
\begin{eqnarray}
任意のB\in\mathcal{B}(\mathbb{R})に対して、X^{-1}(B)\in\mathcal{F}
\end{eqnarray}
　ここで、$(\mathbb{R},\mathcal{B}(\mathbb{R}),P\circ X^{-1})$は確率空間となる\\
　確率変数$X$に対して、分布関数$F_X:\mathbb{R}\rightarrow[0,1]$を次のように定義する
\begin{eqnarray}
F_X(x):=P(X\leq x)
\end{eqnarray}
　今回扱う確率変数は、その確率変数に対応する確率測度$P\circ X^{-1}$が$\mathbb{R}$上のボレル測度$\mu$に対して絶対連続であるものとし、ボレル測度とのラドンニコディム導関数を確率密度関数と呼ぶ\\
　すなわち
\begin{eqnarray}
f_X(x):=\frac{d(P\circ X^{-1})(x)}{d\mu(x)}
\end{eqnarray}
　$\Omega$の部分集合族$\mathcal{G}$が、σ加法族の条件を満たし、さらに$\mathcal{G}\subset\mathcal{F}$が成り立つなら、$\mathcal{G}$を部分σ加法族という\\
　$\Omega$の部分集合族$\mathcal{H}$に対して、$\sigma(\mathcal{H})$で$\mathcal{H}$を含む最小のσ-加法族を表すものとする。\\
　また、確率変数$Y$に対して、$\sigma(Y)$で$Y$を可測にする最小のσ-加法族を表すとする。\\
　\\
　\\
\scalebox{1.1}{2.独立性と条件付き期待値}\\
　集合$A,B\in\mathcal{F}$が独立であるとは、$P(A\cup B)=P(A)P(B)$が成り立つことである\\
　部分σ-加法族$\mathcal{G}_1,\mathcal{G}_2$が独立であるとは、任意の集合$A_1(\in\mathcal{G}_1),A_2(\in\mathcal{G}_2)$が独立となることである\\
　確率変数$X,Y$が独立であるとは、σ-加法族$\sigma(X),\sigma(Y)$が独立となることである\\
　集合$A\in\mathcal{F}$の、$B\in\mathcal{F}$に対する条件付き確率を次のように定義する
\newpage
\section{機械学習入門}
ここからは本格的に機械学習の内容を学んでいく。\\
　工学への応用であるため、数学的な厳密性に関しては数学書に比べればかなり適当なものになるがご容赦願いたい。\\
　\\
\scalebox{1.1}{1.問題設定}\\
ここでは母集団の濃度は可算無限であるとする。（応用上確実に有限なのだが、限りなく大きい場合を扱う）\\
考察に用いたい各データの数値を、特徴量ベクトル$\bm{x}$\\
そして分類したいクラスの集合を$C$と置く
\begin{eqnarray}
\bm{x}&\in & K^d(K^dはスカラー体K_S上の線形位相空間。Kは\mathbb{R}や\mathbb{Q}の部分集合など様々で、具体的な中身は分析対象による)\\
y&\in&C:=[c_1,c_2,...c_n,...](y_iは各クラス名,実数濃度でもよい)
\end{eqnarray}
　これに対して、$\bm{x}$と$y$が紐づけられた濃度$m:=|\mathcal{D}|$のデータ$\mathcal{D}:=[(\bm{x}_1,y_1),......,(\bm{x}_m,y_m)]$が与えられた上で、分類関数$\hat{y}:K^d\rightarrow C$を見繕いたい。\\
この写像がどういった場合に優れていると定義するかは、状況による。（後々具体例を書く）\\
　\\
　確率空間$(\Omega,\mathcal{F},P)$があったとする。ここでの標本空間は上記の母集団と異なることに注意。\\
　確率空間上の可測関数$X:\Omega\rightarrow K^d\times C$について考えたい。ただし、値域となる空間のσ-加法族は$K$が可算集合ならべき集合、$\mathbb{R}$など実数濃度であれば、$\sigma(\mathcal{B}(K^d)\times \mathcal{B}(C))$とする。\\

　データ$\mathcal{D}$の入力は、$m$個の$\Omega\rightarrow K^d\times C$な確率変数として捉えることができる\\
　すなわち、入力データ$\mathcal{D}$に対して、$K^d\times C$‐値確率変数となるような独立同分布な確率変数列$[X_i]^m_{i=1}$が
\begin{eqnarray}
X_i=\mathcal{D}_i=(\bm{x_i},y_i)
\end{eqnarray}
という値になるものと考えればよい\\
 ここで、データ観測前の所持情報を$\mathcal{F}_0$,データ観測後の所持情報を$\mathcal{F}_1$と置く。これはどちらも$\mathcal{F}$の部分σ-加法族で、$\mathcal{F}_0\subset\mathcal{F}_1$であれば、これらは増大情報系の条件を満たし。\\
\begin{eqnarray}
\mathcal{F}_1=\mathcal{F}_0\vee\sigma([X_i]^m_{i=1})
\end{eqnarray}
とおける\\
余談:入力データを確率変数列で捉えるのは、数学的に厳密でありながら難解な数学の概念を持ち出さずに済む方法である。\\
これは、数学上の都合だけかというとそうではなく、$\mathcal{F}_0$と$\mathcal{F}_1$の間に$\mathcal{F}_{\frac{l}{m}},l\in[1,2,......,m]$という部分σ-加法族があると考え、$\mathcal{F}_{\frac{l}{m}}:=\mathcal{F}_0\vee\sigma([X_i]^l_{i=1})$と置けば「通常データは１つずつ取り込まれていき、解析者の保持する情報が増えていく」という実際の状況にも対応させることができる
\newpage
写像$\hat{y}:K^d\rightarrow C$の満たす集合を$Y$とおく。$Y$の位相は後述の$F$を連続関数にする最弱の位相と定義する\\
　\\ 
　入力データ$\mathcal{D}$に対して、最適な写像$\hat{y}$を導き出したい。\\
　ただし、$Y$の任意の元を取ってこれるかと言えばそうではなく、ある程度制約をつけなければ役に立たない（オーバーフィット、具体例を交えて後述）\\
選択されたモデル$M$(詳細は後述)に対して、$Y_M(\subset Y)$でモデルに対応する分類関数の集合とし、この上で最適化を行っていく。\\
　見つけたい最適な分類関数は、このように書ける
\begin{eqnarray}
\tilde{y}:=argmax_{y\in Y_M}[F_\mathcal{D}(y)]
\end{eqnarray}
　ただし、$F_\mathcal{D}$はモデルと入力データから定まる$F_\mathcal{D}:Y\rightarrow\mathbb{R}$の汎関数である（$Y$は必ずしも線形空間ではないので厳密には汎関数ではないが、「引数が関数の関数」という意味でここでは「汎関数」と呼ぶものとする）\\
 　\\
データ$\mathcal{D}$を観測する前のC上の密度関数を事前分布と呼び、観測後の密度関数は事後分布という。\\
$X_C$で$C$への射影、$X_K$で$K^d$への射影を表すものとし、$\tilde{\mathcal{D}}:=\{\omega;[X(\omega)]_{i=1}^m=\mathcal{D}(\omega)\}$とおく
\begin{eqnarray}
f^{\theta_0}_{X_C}(c)&:&事前分布\\
f^{\theta_1}_{X_C}(c|\tilde{\mathcal{D}})&:&事後分布
\end{eqnarray}
　ただし、$f_X(x|A),A\in\mathcal{F}$は条件付き分布とし、確率密度関数$f_X(x)$に対して、$\omega\in A$という情報がある上での$X(\omega)$の密度関数を表すものとする。すなわち
\begin{eqnarray}
f^{\theta_1}_{X_C}(c|\tilde{\mathcal{D}}):=f^{\theta_1}_{X_C(\omega)|\omega\in\tilde{\mathcal{D}}}(c)P(\tilde{\mathcal{D}})^{-1}
\end{eqnarray}
　ただし、$P(\tilde{\mathcal{D}})=0$となる場合(例えば$y=c_i$のとき$\bm{x}$がガウス分布になる場合、必然この測度が$0$になる)は、次のように定義する\\
\begin{eqnarray}
f^{\theta_1}_{X_C}(c_i|\cdot):=\frac{dP(\{\omega;X_C(\omega)=c_i\}\cup\cdot\})}{dP(\{\omega;([X_i]_{i=1}^m)^{-1}(\cdot))\}}
\end{eqnarray}
ただし、データ$\mathcal{D}$に対して、$\{\omega;([X_i]_{i=1}^m)^{-1}(\cdot))\}:=\cap_{i=1}^m\{\omega;X_i^{-1}(\mathcal{D}_i)\}$と定義される\\
この関数については、応用上必要であれば後述の「一般化モンテカルロ法」で数値計算を行う。
　\\
　\\
ここで、事前分布、尤度関数、パラメータの可測性、$Y$の制限、汎関数$F_\mathcal{D}$の組を、「選択されたモデル」といい、$M$と表記する。\\
\begin{eqnarray}
M:=(f^{\theta_0}_{X_C}(c),f^{\tilde{\theta}}_{X_K}(x|\{\omega;X_C(\omega)=c\}), (mesurerable),Y_M,F_\mathcal{D})
\end{eqnarray}
$(K^d,C,M,\mathcal{D})$の組を「汎化機械学習問題」と呼ぶ\\
$(13)$で定義される分類関数$\hat{y}(\bm{x})$を「汎化機械学習問題の解」と呼ぶ\\
　\\
　\\
\scalebox{1.1}{2.各種テクニック}\\
ベイズの定理：\\
事後分布は事前分布とデータの尤度の積に比例する\\
すなわち、ある定数$L$が存在し、任意の$c_i\in C$に対して
\begin{eqnarray}
f^{\theta_1}_{X_C}(c_i|\tilde{\mathcal{D}})=Lf^{\theta_0}_{X_C}(c_i)f^{\tilde{\theta}}_{X_K}(x|\{\omega;X_C(\omega)=c_i\})
\end{eqnarray}
　\\
ここで、事後分布が事前分布のパラメータ違いで同種の分布のとき、事前分布を「共役事前分布」と呼び、事後分布のパラメータは事前分布のパラメータと入力データの関数で書ける
\begin{eqnarray}
すなわち、写像\theta_1:\Theta\rightarrow\Theta が存在し  \theta_1:=\theta_1(\theta,\mathcal{D})
\end{eqnarray}
今後、モデルMにおいて事前分布をこのパラメータの関数で書いた場合、事前分布は共役事前分布であるものとする\\
　\\
　\\
\scalebox{1.1}{3.分類}\\
ここから、このモデルの場合分けでいろいろな手法を定義していく\\
基本的に核となるのは$F_\mathcal{D}$と$Y_M$\\
$C$が可算集合のとき、$Y_M$は往々にして「クラスに対する境界の引き方」で定義される。代わりにモデルの$Y_M$部分に境界の引き方を記入してもよい\\
　\\
1.線形回帰\\
最も基本的なクラスタリングである。\\
$C$の濃度が2で、$Y_M$によって定義される境界が$K^d$の$d-1$次元アフィン部分空間となるモデルを線形回帰モデルと呼ぶ。\\
$C=\{c_1,c_2\}$とする\\
すなわち、境界線は次のように定義される\\
任意の$ij$成分に対して$a_{ij}\in K_S$となる$d$次正方行列$\bm{A}$と$K^d$上のベクトル$\bm{b}$が存在し、
\begin{eqnarray}
\bm{A}\bm{x}+\bm{b}=0
\end{eqnarray}
となる。行列$\bm{A}$とベクトル$b$の選び方は評価汎関数$F_{\mathcal{D}}$の形による。（距離の２乗の最小化であることが多い）\\

2.ロジスティック判別\\
基本はほとんど上記の線形回帰と同じである。\\
ただし、推定の対象は$c_2$である「確率」\\
線形回帰で導き出した関数$f(\bm{x})=\bm{A}\bm{x}+\bm{b}$を用いて
\begin{eqnarray}
log(\frac{y}{1-y})=\bm{A}\bm{x}+\bm{b}
\end{eqnarray}
これで点$\bm{x}$で$c_2$となる確率$p$が求められる

3.決定木\\
線形回帰の拡張である。\\
$C$の濃度は有限であるものとし、$|C|\leq 2^n$となるよう境界線形関数列$\{f_i\}_{i=1}^n$を定義し、それに対する正負の組み合わせでクラスを分類する\\
　\\
4.ランダムフォレスト\\

5.サポートベクターマシン\\


\newpage
\section{ニューラルネットワーク}
２章３の分類に含めても構わなかったのだが、あまりにも長くなるため、ニューラルネットワーク（ディープラーニング）に関してだけは、２個の章に分けて解説する。数学的に最も濃厚かつ難解で、数学科生にとっては一番楽しめる内容になることかと思う。\\
　\\
1.ニューラルネットワークとは\\
 （人工）ニューラルネットワークとは、脳回路の人工的再現である。一つ以上の入力を受けて、値を出力する。ここで言う入力の数とは、先述のモデルで言えば$d$のことである。（以降、他の記号はすべてリセットされたし。$K=\mathbb{R}$とする）\\
　まずは隠れ層が１つである場合を考える\\
　入力層から入力された値のある重みづけを、隠れ層では活性化関数$\eta:\mathbb{R}\rightarrow \mathbb{C}$の引数にする。また、この先にも重みづけを行う。すなわち、入力データを$\bm{x}$,入力後の重みづけベクトルの各成分を$a_j,b_j\in\mathbb{R}$,関数で写像を飛ばしたあとの重みづけベクトルの各成分を$C_j\in\mathbb{C}$とおくと、このニューラルネットワークが示す関数$g:\mathbb{R}\rightarrow\mathbb{C}$は
\begin{eqnarray}
g(\bm{x}):=\Sigma_{j=1}^nC_j\eta(a_jx_j-b_j)
\end{eqnarray}
と表せる。ただし$n$は隠れ層のノード数である。\\
　入力に対して、正しい出力がわかっている状況で正しい出力が出るようにするにはこの重みづけのパラメータを調整すればよい。そのようにして、正しい関数$f:\mathbb{R}^d\rightarrow\mathbb{C}$の近似$g$を探索するのが「ニューラルネットワーク」である。\\
　ところで、上記の式は、ノード数$n\rightarrow\infty$とすることで、積分にすることができる。すなわち
\begin{eqnarray}
g(\bm{x}):=\int_{\mathbb{Y}^{d+1}}T(a,b)\eta(a\cdot x-b)d\mu(a,b)
\end{eqnarray}
　ただし、$T(a,b)$は、複素数列$C_j$の連続版に当たる写像$T:\mathbb{Y}^{d+1}\rightarrow\mathbb{C}$であり、$a\cdot x$はユークリッド内積\\
　このような積分の存在は仮定する（$T,\eta$に対してこの積分が収束するような測度$\mu$のみを扱う）。また、$\mu$にルベーグ測度との絶対連続性を課さなければ、離散化はこの特別な場合として書ける。\\
　\\
　\\
2.リッジレット作用素\\
ではその関数$T:\mathbb{Y}^{d+1}\rightarrow\mathbb{C}$を求めるにはどうすればいいか。これは実は既存研究で解析的に書ける。\\
　\\
関数$f\in L^1(\mathbb{R}^{d};\mathbb{C})$の、$\psi\in L^\infty(\mathbb{R};\mathbb{C})$に対するリッジレット作用素$\mathcal{R}_\psi$を次のように定義する
\begin{eqnarray}
(\mathcal{R}_\psi f)(a,b):=\int_{\mathbb{R}^{d}}f(\bm{x})\overline{\psi(a\cdot\bm{x}-b)}|a|^sd\bm{x}
\end{eqnarray}
　\\
次に、双対リッジレット作用素を定義する。\\
$\mathcal{R}^*_\cdot \cdot:L^1(\mathbb{Y}^{d+1};\mathbb{C},|a|^{-s}dadb)\times L^\infty(\mathbb{R};\mathbb{C})\rightarrow L^\infty(\mathbb{R}^d)$を定義する
\begin{eqnarray}
(\mathcal{R}^*_\eta T)(\bm{x}):=\int_{Y^{d+1}}T(a,b)\eta(a\cdot\bm{x}-b)|a|^{-s}d\mu(a,b)
\end{eqnarray}
これは適当な条件（要検証）の元で、リッジレット作用素$\mathcal{R}_\eta$の双対作用素となる。\\
　\\
定義：\\
次の積分が有界で非零のとき、$\psi,\eta$に対する核が許容的であるという
\begin{eqnarray}
K_{\psi,\eta}:=(2\pi)^{d-1}\int_\mathbb{R}\frac{\overline{\hat{\psi}(\zeta)}\eta{\zeta}}{|\zeta|^d}d\zeta
\end{eqnarray}
　\\
定理：再構成公式\\
$\psi,\eta$は上述の関数と同一集合上の元であるとし、また核は許容的であるとする。このとき、次の式が成り立つ。
\begin{eqnarray}
f(\bm{x})=\int_{\mathbb{Y}^{d+1}}(\mathcal{R}_\psi f)(a,b)\eta(a\cdot\bm{x}-b)dadb
\end{eqnarray}
よって、$T:=\mathcal{R}_\psi f$と置くと、この積分的ニューラルネットワークは関数$f$と同じになる。実際のニューラルネットワークではこれを離散化するため、関数$f$の近似となる。\\
　\\
　\\
3.離散化\\
ここまで、連続的に三層パーセプトロンを考察してきたが、実際のニューラルネットワークにおいて、隠れ層のノード数は有限である。そのため、中間層素子のの離散化を考える必要がある。\\
　\\
活性化関数$\eta$に対して、すべての中間層素子の集合を「辞書」と呼ぶ。
\begin{eqnarray}
\mathcal{D}:=\{h(\cdot;a,b)|(a,b)\in\mathbb{Y}^{d+1}\}
\end{eqnarray}
ただし、$h(x;a,b):=\eta(a\cdot\bm{x}-b)$\\
　\\
辞書の部分集合$D$（以下、部分辞書と呼称する）と、関数$T:\mathbb{Y}^{d+1}\rightarrow\mathbb{C}$の内積を次のように定義する
\begin{eqnarray}
(DT)(\bm{x}):=\int_D T(a,b)h(\bm{x};a,b)d\mu(a,b)
\end{eqnarray}
ただし、この積分は実際には適宜局所座標系をとらないと面倒である。\\
ここで、このような形でかけるとき、これは実際に実装されるニューラルネットワークの挙動になる。
\begin{eqnarray}
(DT)(\bm{x})=\Sigma_D T(a,b)h(\bm{x};a,b)
\end{eqnarray}
部分辞書$D$が可算集合であるとき、これを「離散化」という。\\
離散化の評価は必要に応じて様々な関数空間のノルムで行われる。\\
困ったことに、$\mathcal{D}$の閉方はヒルベルト空間であるとは限らないで、一般には正規直交基底をとれない。
\newpage
4.一般化されたリッジレット作用素\\
実際のニューラルネットワークでは、様々な関数が活性化関数として使われる。\\
ここで問題になってくるのは、ReLU（$\eta(x):=x_+$）のような、「実際によく使われるが上記の定義では定式化できない（$L^\infty$の元ではない）」場合である。\\
そこで、この章ではリッジレット作用素を超関数論で一般化し、活用範囲を大幅に広げる

　\\
まず、上述の$(a,b)\in\mathbb{Y}^{d+1}$を極座標変換する
\begin{eqnarray}
u:=a/|a|,\alpha:=1/|a|,\beta:=b/|a|
\end{eqnarray}
特に誤解の恐れがない状況では、座標系の取り方によらずパラメータの空間を$\mathbb{Y}$と表記する\\
　\\
リッジレット変換を次のように超関数として定義する\\
$f\in\mathcal{X}(\mathbb{R}^d)$の$\psi\in\mathcal{Z}(\mathbb{R})$（空間の定義は後述）によるリッジレット変換を、こう定義する
\begin{eqnarray}
(\mathcal{R}_\psi f)(u,\alpha,\beta):=\overline{\int_{\mathbb{R}}\overline{Rf(u,az+\beta)}\psi(z)dz}
\end{eqnarray}
ただし、$R$はラドン変換
\includegraphics[width=10cm]{DNYwj57VoAEsMfG.jpg}\\
（ここに自分で図を書いておく）\\

定理\\
この組み合わせにおいて、作用素$\mathcal{R}_\cdot \cdot:\mathcal{X}(\mathbb{R}^d)\times\mathcal{Z}(\mathbb{R})\rightarrow\mathcal{Y}(\mathbb{Y}^{d+1})$は、双線形写像である\\
　\\
定理\\
$\mathcal{X}=L^1$とし、$\psi\in\mathcal{S}(\mathbb{R})$を固定する。このときリッジレット作用素$\mathcal{R}_\psi$は有界作用素\\
　\\
定義\\
関数$T\in\mathcal{Y}(\mathbb{Y}^{d+1})$の、活性化関数$\eta\in\mathcal{W}(\mathbb{R})$による双対リッジレット変換を次のように定義する
\begin{eqnarray}
(\mathcal{R}^*_\eta T)(\bm{x}):=lim_{\delta\rightarrow\infty,\varepsilon\rightarrow0}\int_{\mathbb{S}^{d-1}}\int^\delta_\varepsilon\int_{\mathbb{R}}T(u,\alpha,u\cdot x-\alpha z)\eta(z)\frac{dzd\alpha du}{\alpha^d}
\end{eqnarray}
$\mathcal{R}^*_\eta $は、存在すれば$\mathcal{R}_\eta$の双対作用素
　\\
活性化関数は超関数でもよい。次に許容条件について定義する\\
　\\
定義\\
$(\psi,\eta)\in\mathcal{S}\times \mathcal{S}`$が許容的であるとは、任意の原点を含む近傍$\Omega$に対し、次の積分が0でない値に収束し$\hat{\eta}$が局所可積分であることを言う
\begin{eqnarray}
K_{\psi,\eta}:=(2\pi)^{d-1}(\int_{\Omega/\{0\}}+\int_{\mathbb{R}/\Omega})\frac{\overline{\hat{\psi}(\zeta)}\hat{\eta}(\zeta)}{|\zeta|}
\end{eqnarray}
また、この二つの関数が等しいとき、これを自己許容的という\\
　\\
定理（リッジレット変換の$L^2$拡張性）\\
$\psi\in\mathcal{S}$は自己許容的であるとし、$K_{\psi,\psi}=1$とする。このとき、$L^1\cap L^2(\mathbb{R}^d)$上のリッジレット作用素は$L^2(\mathbb{R}^d)$上の作用素に一意に拡大でき、$||\mathcal{R}_\psi f||^2=||f||^2$（等距離写像）\\
　\\
具体的な再構成公式や離散化などは参考文献を参照されたし
　\\
　\\
　\\



\newpage
\section{ディープラーニング}
いよいよここから、第三次人工知能ブームを巻き起こしたとして巷で話題の深層学習に入り込んでいく。数学的なレベルは前章よりも大幅に上がるので、前章で知識不足を感じた方は関数解析や確率解析,圏論の書籍を片手に読むことを勧める。\\
　\\
深層学習とは\\
前章で扱ったニューラルネットワークは、入力層と隠れ層と出力層が１つずつであったが、ここからは隠れ層が２つ以上のもの、つまり$n$層パーセプトロン$(n>3)$を扱っていく\\
　\\
深層学習の積分表現の課題点\\
積分が入れ子構造になり非常にややこしい\\
 
デノイジング・オートエンコーダ（DAE）とは\\
まず、オートエンコーダについて解説する。ニューラルネットは往々にして「次元削減をしたうえで復元する機構」と言われる。次元削減する機構を「エンコーダ」、次元復元する機構を「デコーダ」という。\\
学習の前に、恒等写像を学習させる機構を「オートエンコーダ」という\\
すなわち、関数$f:x\rightarrow x$を近似する。
デノイジングでは、この入力データにあえてノイズを付与して学習させることで、ノイズに対して頑健なニューラルネットワークを作ることができる。\\
　\\
まずは３層パーセプトロンについて考察する。入力データ$X\in\mathbb{R}^d$は確率密度関数$\pi_0(x)$で定まる確率変数であるとする。ここに$d$次元独立正規分布(分散$t$)の確率変数$\varepsilon$を加える。
\begin{eqnarray}
\tilde{X}:=X+\varepsilon
\end{eqnarray}
そして、2章のモデルに倣い、次の関数を最大化することを考えたい。（2乗誤差最小化）
\begin{eqnarray}
F(g):=-E[|X-g(\tilde{X})|^2]
\end{eqnarray}
中間層の写像$h$と、出力層の写像$k$を用いて、$g=h\circ k$と書ける。この記法は後々深層学習を考えるにあたって非常に重要になる\\
　\\
定理：\\
上記の最適化問題の解は次の輸送写像になる（ジェームスシュタイン推定量？）
\begin{eqnarray}
g(x):=x+t\nabla log[K(x,\tilde{x},t/2)*\pi_0(x)]
\end{eqnarray}
ただし、$K$は熱核で、$K(x,y,t):=(4\pi t)^{-d/2}exp[-|x-y|^2/4t]$となる。つまり、$K(\cdot,\cdot,t/2)$で、時刻$t$のブラウン運動の推移確率密度となる\\
実のところ、この畳み込みは偏微分作用素$e^{t/2\Delta}$の作用と同じである（要検証）\\
この輸送写像は、もっと拡張して楕円型作用素$L$に対して定義できる\\
　\\
定義：\\
$L$を楕円型偏微分作用素。関数$K_t(x,y;L)$を、偏微分方程式$\partial_t u(t,x-y)=Lu(t,x-y)$の解とする。この時、異方性DAEを次のように定義できる
\begin{eqnarray}
\Phi_t(x;L):&=&x+t\nabla log[e^{tL}\pi_0(x)]\nonumber\\
&=&x+t\nabla log[\int_{\mathbb{R}^d}K_t(x,y;L)\pi_0(y)dy]
\end{eqnarray}
これを「ニューラルネットワークの輸送表現」という。つまり、三層パーセプトロンのデノイジングオートエンコーダーによる学習をこの関数で表現できる。\\
　\\
確率密度関数$\pi_0$から生成される確率測度を$P_0$と置く（この確率測度はa.eで一意）。これがルベーグ測度に絶対連続なら、下記の式で定義される$P_t$もすべてルベーグ測度と絶対連続である。
\begin{eqnarray}
P_t:=P_0\circ\Phi_t^{-1}
\end{eqnarray}
ここで、$P_0$が正規分布、$L$がラプラシアンであるとき、連続DAEに対する測度$\mu$（詳細は後述）はWiener測度である。\\
ちなみに、$P_t$のルベーグ測度とのラドンニコディム導関数を$\pi_t$と置くと、次の式が成り立つ
\begin{eqnarray}
\partial_t \pi_t|_{t=0}=-\Delta \pi_0
\end{eqnarray}
　\\
2.深層デノイジング・オートエンコーダーについて\\
いよいよ深層学習を対象として論理展開をしていく。以後、使用する楕円型偏微分作用素はラプラシアンとし、$L$は隠れ層の数を表す整数であるものとする\\
終端時刻は$T$、これを$L$分割し、時間の変分は$\tau_l$（停止時刻とは無関係であることに注意）であらわされるとする。そして、現在時刻を$t_l(:=\tau_0+\tau_1+......+\tau_{l-1})$と表記する。各DAEで加えられるノイズの確率分布は、$\mathcal{N}(0,\tau_lI)$であるとする\\
　\\
そして、$l+1$番目のDAEで計算された写像を$\Phi_l:\mathbb{R}^d\rightarrow\mathbb{R}^d$と表記する\\
ここで、最初の入力データを$X_0$と置くと、これは初記の通り確率密度関数$\pi_0$に従う$\mathbb{R}^d$上の確率変数となる。ここで、確率測度$P_{t_1}:=P_0\circ\Phi_0$を定義し、これと同様に帰納的に$P_{L+1}$まで定義していく\\
また、次のように合成DAEを定義する
\begin{eqnarray}
\Phi^T_{0;L}:=\Phi_L\circ......\circ\Phi_1\circ\Phi_0
\end{eqnarray}
これを用いて、次のように連続DAEが定義できる\\
定義:連続DAE\\
次の連続力学系の解作用素$\phi_t$を連続DAEと呼ぶ
\begin{eqnarray}
dX_t=\nabla log\pi_t[X_t]dt\\
ただし、\pi_tは確率測度P_t:=P_0\circ\phi_t\nonumber
\end{eqnarray}
　\\
極限の存在と一意性は次の定理よりいえる\\ 
　\\
定理：\\
$log \pi_0$は局所リプシッツ連続であるとする。このとき
\begin{eqnarray}
\Phi^T_{0:L}\rightarrow \phi_T ,L\rightarrow \infty
\end{eqnarray}
結局のところ、連続DAEとは、標準ブラウン運動$B_t$を用いて、$x+B_{T-t}$と表記される逆向きブラウン運動に対する、$x$のジェームスシュタイン推定をニュ－ラルネットワークに学習させる機構である。実際の深層DAEはその離散化となる。\\
深層化する意味もここで説明できる。単純にDAEのサンプリングによるドリフト推定するにあたって、観測点を増やしたほうが精度が上がるに決まっている。それと同じことがDAEで起きているのである。\\
　\\
3．積層DAEと合成DAE\\
ここからは、深層DAEについて圏論的・微分幾何学的に考察する



\newpage
参考文献\\
http://www2.itc.kansai-u.ac.jp/~afujioka/2014/ig/141112ig.pdf
https://datumstudio.jp/blog/\\
http://sinhrks.hatenablog.com/entry/2014/12/15/081113
実際の伊藤西尾の定理とはだいぶ形が違う。\\
実際には、「$[0,T]$上のカメロン-マルティン部分空間の正規直交基底列$h_k$と、標準正規分布に従う確率変数列$X_k$に対して、$S_n:=\Sigma^n_{k=0}h_kX_k$から誘導される測度がWiener測度に弱収束する」という定理である。
　\\
　




\end{document}
