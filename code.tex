\documentclass{jsarticle}
\usepackage[dvipdfmx]{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{type1cm}
\usepackage{bm}
\title{機械学習原論}
\author{}
\begin{document}
\maketitle
\section{確率論の基礎の復習}
この章では、機械学習の数学的定式化に必要な確率論の基礎中の基礎を復習する。\\
　目的はあくまで機械学習への応用であるため、細々とした議論は行わず、機械学習の基礎を定式化するために必要な最低限の項目だけ解説する。\\
　ここの内容で対応できるのは2章まで。3章以降の数学に関しては、各自関数解析や確率解析の書物を参照すること　\\
 　\\
\scalebox{1.1}{1.確率空間と確率変数}\\
　標本空間$\Omega$に対し、次を満たす部分集合族$\mathcal{F}$をσ加法族と呼ぶ
\begin{eqnarray}
&1.&\phi,\Omega\in\mathcal{F}\\
&2.&A\in\mathcal{F}\rightarrow A^c\in\mathcal{F}\\
&3.&可算個の\Omega の部分集合 A_1,A_2,......,があり、任意のnに対してA_n\in\mathcal{F}なら、\cup^\infty_{n=1} A_n\in\mathcal{F}
\end{eqnarray}
　写像$P:\mathcal{F}\rightarrow[0,1]$が次を満たすとき、$P$を確率測度という
\begin{eqnarray}
&1.&P(\Omega)=1\\
&2.&互いに素な可算個の集合A_1,A_2,...,\in\mathcal{F}に対し、P(\cup^\infty_{n=1}A_n)=\Sigma^\infty_{n=1}P(A_n)
\end{eqnarray}
　この$(\Omega,\mathcal{F},P)$の組を、確率空間と呼ぶ\\
　$\mathcal{B}(\mathbb{R})$で、$\mathbb{R}$の開集合をすべて含む最小のσ加法族を表し、ボレル集合族と呼ぶ。\\
　写像$X:\Omega\rightarrow\mathbb{R}$が次の条件を満たすとき、（1次元実）確率変数であるという
\begin{eqnarray}
任意のB\in\mathcal{B}(\mathbb{R})に対して、X^{-1}(B)\in\mathcal{F}
\end{eqnarray}
　ここで、$(\mathbb{R},\mathcal{B}(\mathbb{R}),P\circ X^{-1})$は確率空間となる\\
　確率変数$X$に対して、分布関数$F_X:\mathbb{R}\rightarrow[0,1]$を次のように定義する
\begin{eqnarray}
F_X(x):=P(X\leq x)
\end{eqnarray}
　今回扱う確率変数は、その確率変数に対応する確率測度$P\circ X^{-1}$が$\mathbb{R}$上のボレル測度$\mu$に対して絶対連続であるものとし、ボレル測度とのラドンニコディム導関数を確率密度関数と呼ぶ\\
　すなわち
\begin{eqnarray}
f_X(x):=\frac{d(P\circ X^{-1})(x)}{d\mu(x)}
\end{eqnarray}
　$\Omega$の部分集合族$\mathcal{G}$が、σ加法族の条件を満たし、さらに$\mathcal{G}\subset\mathcal{F}$が成り立つなら、$\mathcal{G}$を部分σ加法族という\\
　$\Omega$の部分集合族$\mathcal{H}$に対して、$\sigma(\mathcal{H})$で$\mathcal{H}$を含む最小のσ-加法族を表すものとする。\\
　また、確率変数$Y$に対して、$\sigma(Y)$で$Y$を可測にする最小のσ-加法族を表すとする。\\
　\\
　\\
\scalebox{1.1}{2.独立性と条件付き期待値}\\
　集合$A,B\in\mathcal{F}$が独立であるとは、$P(A\cup B)=P(A)P(B)$が成り立つことである。\\
　部分σ-加法族$\mathcal{G}_1,\mathcal{G}_2$が独立であるとは、任意の集合$A_1(\in\mathcal{G}_1),A_2(\in\mathcal{G}_2)$が独立となることである。\\
　確率変数$X,Y$が独立であるとは、σ-加法族$\sigma(X),\sigma(Y)$が独立となることである。\\
　集合$A\in\mathcal{F}$の、$B\in\mathcal{F}$に対する条件付き確率を次のように定義する。（確率論入門は後日書く）
\newpage
\section{機械学習入門}
ここからは本格的に機械学習の内容を学んでいく。\\
　工学への応用であるため、数学的な厳密性に関しては数学書に比べればかなり適当なものになるがご容赦願いたい。\\
　\\
　\\
\scalebox{1.1}{1.問題設定}\\
　機械学習のタスクの多くは、次のような問題設定で一般化できる。\\
　「分類」や「回帰」、「次元削減」などの違いについてはこの段階では区別しないのであしからず。\\
　ちなみに、これで書けない機械学習のタスクも、その大半はこの問題設定のわずかな改変で記述できる。\\
　\\
　確率空間$(\Omega,\mathcal{F},P)$を定義する。\\
　考察に用いたい特徴量の集合を特徴量ベクトル$\bm{x}$。それに対して教師データを$y$と表記する。
\begin{eqnarray}
\bm{x}&\in & \mathbb{R}^d\nonumber\\
y&\in&\mathbb{R}^n\nonumber
\end{eqnarray}
　これに対して、$\bm{x}$と$y$が紐づけられたm個のデータ$\mathcal{D}:=[X_1,X_2,......,X_m]:=[(\bm{x}_1,y_1),......,(\bm{x}_m,y_nm)]$が与えられた上で、できるだけ良い分類関数$\hat{y}:\mathbb{R}^d\rightarrow \mathbb{R}^n$を見繕いたい。\\
　各$X_i$は、確率空間上の独立同分布な確率変数$X_i:\Omega\rightarrow \mathbb{R}^d\times \mathbb{R}^m$である。ただし、値域となる空間のσ-加法族はボレル集合族をとるものとする。\\
　ここで、データ観測前の所持情報を$\mathcal{F}_0$,データ観測後の所持情報を$\mathcal{F}_1$と置く。これはどちらも$\mathcal{F}$の部分σ-加法族で、$\mathcal{F}_0\subset\mathcal{F}_1$であれば、これらは増大情報系の条件を満たし、次のように記述できる。
\begin{eqnarray}
\mathcal{F}_1=\mathcal{F}_0\vee\sigma([X_i]^m_{i=1})
\end{eqnarray}
写像$\hat{y}:\mathbb{R}^d\rightarrow \mathbb{R}^n$の満たす集合を$Y$とおく。$Y$の位相は後述の$F$を連続関数にする最弱の位相と定義する。\\
　入力データ$\mathcal{D}$に対して、最適な写像$\hat{y}$を導き出したい。\\
　ただし、$Y$の任意の元を取ってこれるかと言えばそうではなく、ある程度制約をつけなければ役に立たない。\\
　選択されたモデル$M$(詳細は後述)に対して、$Y_M(\subset Y)$でモデルに対応する関数の集合とし、この上で最適化を行っていく。\\
\begin{eqnarray}
\tilde{y}:=argmin_{y\in Y_M}[F_\mathcal{D}(y)]
\end{eqnarray}
　ただし、$F_\mathcal{D}$はモデルと入力データから定まる$F_\mathcal{D}:Y\rightarrow\mathbb{R}$の汎関数である。\\
　そして、後述の様々な技法の中から使用されるものを「選択されたモデル$M$」と呼ぶものとする。$(\mathbb{R}^d,\mathbb{R}^n,\mathcal{D},M)$の組を「汎化機械学習問題」と呼び、そこから導き出された最適な写像$\hat{y}$を、「汎化機械学習問題の解」と呼ぶ。

\newpage
\scalebox{1.1}{2.モデル設定}\\
　ここでは、ベイズとニューラルネットワークを除いたモデルについて解説していく。（ニューラルネットワークが絡むと、絡んでいない場合と比べて数学的な議論が圧倒的に濃くなるため、4章以降はすべてニューラルネットワークの話である。ベイズは語る内容が多い）\\
　「これ機械学習じゃなくて普通の統計学じゃないか」と思われるであろう要素を多分に含んでいるが、そもそも機械学習と統計学の明確な線引きは不可能であることに注意していただきたい。\\
　\\
1.線形識別・線形回帰(2クラス)\\
　特徴量ベクトルの空間$\mathbb{R}^d$を２つに分類する。教師データの集合は$\mathbb{R}^n,n=1$として、$Y_M$を次のように定義する。
\begin{eqnarray}
Y_M:=\{\tilde{y}\in Y|A\bm{x}+b=0を満たすアフィン部分空間がd-1次元(別名「超平面」)になるA^t\in \mathbb{R}^d,b\in\mathbb{R}^n \}
\end{eqnarray}
また、これらの関数の評価関数はこのように定義できる。
\begin{eqnarray}
F_\mathcal{D}(\tilde{y}):&=&\{各データの、上述のアフィン部分空間までの距離の和\}\nonumber\\
&=&\Sigma_{i=1}^m |X_i-\rho(X_i)|^2
\end{eqnarray}
ただし、$\rho(X)$で、$X$のアフィン部分空間への射影であるとする。\\
　こうして求められた最適な$A.b$に対して、$Ax+b>0$か$Ax+b<0$かでデータを二つのクラスに分類する。
　\\
2.ロジスティック回帰\\
　基本的には2クラスの線形回帰と同じく、$\mathbb{R}^d$を２つにわけるのだが、今回はすっぱりと分けてしまうわけではなく、片方のクラスに属する「確率」を求める。\\
 
　線形回帰は$f(x):=Ax+b$の$A,b$はすべて定数倍に対して同値だったが、今回はそうもいかない。

3.線形回帰(多クラス)

4.決定木

5.ランダムフォレスト

6.サポートベクターマシン

7.主成分分析


\newpage
\scalebox{1.1}{3.実装について}\\
\newpage
\section{ベイズ学習}
　ベイズ統計に深入りすると一生終わらなくなるので、とりあえず機械学習の場でよく使われるベイズ学習について簡単に解説する。\\

\newpage
\section{ニューラルネットワーク}
　本書のメインである。



\newpage
\begin{thebibliography}{99}
  \bibitem{キー1} 確率論(実教理工学全書),西尾真紀子(1978)
  \bibitem{キー2} Machine Learning: A Probabilistic Perspective (Adaptive Computation and Machine Learning series),Kevin.P.Murphy (2012)

\end{thebibliography}



\newpage

　\\

　\\

線形回帰で導き出した関数$f(\bm{x})=\bm{A}\bm{x}+\bm{b}$を用いて
\begin{eqnarray}
log(\frac{y}{1-y})=\bm{A}\bm{x}+\bm{b}
\end{eqnarray}
これで点$\bm{x}$で$c_2$となる確率$p$が求められる

3.決定木\\
線形回帰の拡張である。\\
$C$の濃度は有限であるものとし、$|C|\leq 2^n$となるよう境界線形関数列$\{f_i\}_{i=1}^n$を定義し、それに対する正負の組み合わせでクラスを分類する\\
　\\
4.ランダムフォレスト\\

5.サポートベクターマシン\\


\newpage
\section{ニューラルネットワーク}
２章３の分類に含めても構わなかったのだが、あまりにも長くなるため、ニューラルネットワーク（ディープラーニング）に関してだけは、２個の章に分けて解説する。数学的に最も濃厚かつ難解で、数学科生にとっては一番楽しめる内容になることかと思う。\\
　\\
1.ニューラルネットワークとは\\
 （人工）ニューラルネットワークとは、脳回路の人工的再現である。一つ以上の入力を受けて、値を出力する。ここで言う入力の数とは、先述のモデルで言えば$d$のことである。（以降、他の記号はすべてリセットされたし。$K=\mathbb{R}$とする）\\
　まずは隠れ層が１つである場合を考える\\
　入力層から入力された値のある重みづけを、隠れ層では活性化関数$\eta:\mathbb{R}\rightarrow \mathbb{C}$の引数にする。また、この先にも重みづけを行う。すなわち、入力データを$\bm{x}$,入力後の重みづけベクトルの各成分を$a_j,b_j\in\mathbb{R}$,関数で写像を飛ばしたあとの重みづけベクトルの各成分を$C_j\in\mathbb{C}$とおくと、このニューラルネットワークが示す関数$g:\mathbb{R}\rightarrow\mathbb{C}$は
\begin{eqnarray}
g(\bm{x}):=\Sigma_{j=1}^nC_j\eta(a_jx_j-b_j)
\end{eqnarray}
と表せる。ただし$n$は隠れ層のノード数である。\\
　入力に対して、正しい出力がわかっている状況で正しい出力が出るようにするにはこの重みづけのパラメータを調整すればよい。そのようにして、正しい関数$f:\mathbb{R}^d\rightarrow\mathbb{C}$の近似$g$を探索するのが「ニューラルネットワーク」である。\\
　ところで、上記の式は、ノード数$n\rightarrow\infty$とすることで、積分にすることができる。すなわち
\begin{eqnarray}
g(\bm{x}):=\int_{\mathbb{Y}^{d+1}}T(a,b)\eta(a\cdot x-b)d\mu(a,b)
\end{eqnarray}
　ただし、$T(a,b)$は、複素数列$C_j$の連続版に当たる写像$T:\mathbb{Y}^{d+1}\rightarrow\mathbb{C}$であり、$a\cdot x$はユークリッド内積\\
　このような積分の存在は仮定する（$T,\eta$に対してこの積分が収束するような測度$\mu$のみを扱う）。また、$\mu$にルベーグ測度との絶対連続性を課さなければ、離散化はこの特別な場合として書ける。\\
　\\
　\\
2.リッジレット作用素\\
ではその関数$T:\mathbb{Y}^{d+1}\rightarrow\mathbb{C}$を求めるにはどうすればいいか。これは実は既存研究で解析的に書ける。\\
　\\
関数$f\in L^1(\mathbb{R}^{d};\mathbb{C})$の、$\psi\in L^\infty(\mathbb{R};\mathbb{C})$に対するリッジレット作用素$\mathcal{R}_\psi$を次のように定義する
\begin{eqnarray}
(\mathcal{R}_\psi f)(a,b):=\int_{\mathbb{R}^{d}}f(\bm{x})\overline{\psi(a\cdot\bm{x}-b)}|a|^sd\bm{x}
\end{eqnarray}
　\\
次に、双対リッジレット作用素を定義する。\\
$\mathcal{R}^*_\cdot \cdot:L^1(\mathbb{Y}^{d+1};\mathbb{C},|a|^{-s}dadb)\times L^\infty(\mathbb{R};\mathbb{C})\rightarrow L^\infty(\mathbb{R}^d)$を定義する
\begin{eqnarray}
(\mathcal{R}^*_\eta T)(\bm{x}):=\int_{Y^{d+1}}T(a,b)\eta(a\cdot\bm{x}-b)|a|^{-s}d\mu(a,b)
\end{eqnarray}
これは適当な条件（要検証）の元で、リッジレット作用素$\mathcal{R}_\eta$の双対作用素となる。\\
　\\
定義：\\
次の積分が有界で非零のとき、$\psi,\eta$に対する核が許容的であるという
\begin{eqnarray}
K_{\psi,\eta}:=(2\pi)^{d-1}\int_\mathbb{R}\frac{\overline{\hat{\psi}(\zeta)}\eta{\zeta}}{|\zeta|^d}d\zeta
\end{eqnarray}
　\\
定理：再構成公式\\
$\psi,\eta$は上述の関数と同一集合上の元であるとし、また核は許容的であるとする。このとき、次の式が成り立つ。
\begin{eqnarray}
f(\bm{x})=\int_{\mathbb{Y}^{d+1}}(\mathcal{R}_\psi f)(a,b)\eta(a\cdot\bm{x}-b)dadb
\end{eqnarray}
よって、$T:=\mathcal{R}_\psi f$と置くと、この積分的ニューラルネットワークは関数$f$と同じになる。実際のニューラルネットワークではこれを離散化するため、関数$f$の近似となる。\\
　\\
　\\
3.離散化\\
ここまで、連続的に三層パーセプトロンを考察してきたが、実際のニューラルネットワークにおいて、隠れ層のノード数は有限である。そのため、中間層素子のの離散化を考える必要がある。\\
　\\
活性化関数$\eta$に対して、すべての中間層素子の集合を「辞書」と呼ぶ。
\begin{eqnarray}
\mathcal{D}:=\{h(\cdot;a,b)|(a,b)\in\mathbb{Y}^{d+1}\}
\end{eqnarray}
ただし、$h(x;a,b):=\eta(a\cdot\bm{x}-b)$\\
　\\
辞書の部分集合$D$（以下、部分辞書と呼称する）と、関数$T:\mathbb{Y}^{d+1}\rightarrow\mathbb{C}$の内積を次のように定義する
\begin{eqnarray}
(DT)(\bm{x}):=\int_D T(a,b)h(\bm{x};a,b)d\mu(a,b)
\end{eqnarray}
ただし、この積分は実際には適宜局所座標系をとらないと面倒である。\\
ここで、このような形でかけるとき、これは実際に実装されるニューラルネットワークの挙動になる。
\begin{eqnarray}
(DT)(\bm{x})=\Sigma_D T(a,b)h(\bm{x};a,b)
\end{eqnarray}
部分辞書$D$が可算集合であるとき、これを「離散化」という。\\
離散化の評価は必要に応じて様々な関数空間のノルムで行われる。\\
困ったことに、$\mathcal{D}$の閉方はヒルベルト空間であるとは限らないで、一般には正規直交基底をとれない。
\newpage
4.一般化されたリッジレット作用素\\
実際のニューラルネットワークでは、様々な関数が活性化関数として使われる。\\
ここで問題になってくるのは、ReLU（$\eta(x):=x_+$）のような、「実際によく使われるが上記の定義では定式化できない（$L^\infty$の元ではない）」場合である。\\
そこで、この章ではリッジレット作用素を超関数論で一般化し、活用範囲を大幅に広げる

　\\
まず、上述の$(a,b)\in\mathbb{Y}^{d+1}$を極座標変換する
\begin{eqnarray}
u:=a/|a|,\alpha:=1/|a|,\beta:=b/|a|
\end{eqnarray}
特に誤解の恐れがない状況では、座標系の取り方によらずパラメータの空間を$\mathbb{Y}$と表記する\\
　\\
リッジレット変換を次のように超関数として定義する\\
$f\in\mathcal{X}(\mathbb{R}^d)$の$\psi\in\mathcal{Z}(\mathbb{R})$（空間の定義は後述）によるリッジレット変換を、こう定義する
\begin{eqnarray}
(\mathcal{R}_\psi f)(u,\alpha,\beta):=\overline{\int_{\mathbb{R}}\overline{Rf(u,az+\beta)}\psi(z)dz}
\end{eqnarray}
ただし、$R$はラドン変換
\includegraphics[width=10cm]{DNYwj57VoAEsMfG.jpg}\\
（ここに自分で図を書いておく）\\

定理\\
この組み合わせにおいて、作用素$\mathcal{R}_\cdot \cdot:\mathcal{X}(\mathbb{R}^d)\times\mathcal{Z}(\mathbb{R})\rightarrow\mathcal{Y}(\mathbb{Y}^{d+1})$は、双線形写像である\\
　\\
定理\\
$\mathcal{X}=L^1$とし、$\psi\in\mathcal{S}(\mathbb{R})$を固定する。このときリッジレット作用素$\mathcal{R}_\psi$は有界作用素\\
　\\
定義\\
関数$T\in\mathcal{Y}(\mathbb{Y}^{d+1})$の、活性化関数$\eta\in\mathcal{W}(\mathbb{R})$による双対リッジレット変換を次のように定義する
\begin{eqnarray}
(\mathcal{R}^*_\eta T)(\bm{x}):=lim_{\delta\rightarrow\infty,\varepsilon\rightarrow0}\int_{\mathbb{S}^{d-1}}\int^\delta_\varepsilon\int_{\mathbb{R}}T(u,\alpha,u\cdot x-\alpha z)\eta(z)\frac{dzd\alpha du}{\alpha^d}
\end{eqnarray}
$\mathcal{R}^*_\eta $は、存在すれば$\mathcal{R}_\eta$の双対作用素
　\\
活性化関数は超関数でもよい。次に許容条件について定義する\\
　\\
定義\\
$(\psi,\eta)\in\mathcal{S}\times \mathcal{S}`$が許容的であるとは、任意の原点を含む近傍$\Omega$に対し、次の積分が0でない値に収束し$\hat{\eta}$が局所可積分であることを言う
\begin{eqnarray}
K_{\psi,\eta}:=(2\pi)^{d-1}(\int_{\Omega/\{0\}}+\int_{\mathbb{R}/\Omega})\frac{\overline{\hat{\psi}(\zeta)}\hat{\eta}(\zeta)}{|\zeta|}
\end{eqnarray}
また、この二つの関数が等しいとき、これを自己許容的という\\
　\\
定理（リッジレット変換の$L^2$拡張性）\\
$\psi\in\mathcal{S}$は自己許容的であるとし、$K_{\psi,\psi}=1$とする。このとき、$L^1\cap L^2(\mathbb{R}^d)$上のリッジレット作用素は$L^2(\mathbb{R}^d)$上の作用素に一意に拡大でき、$||\mathcal{R}_\psi f||^2=||f||^2$（等距離写像）\\
　\\
具体的な再構成公式や離散化などは参考文献を参照されたし
　\\
5.実装について\\
一般には、誤差電波法が用いられる。
リッジレット関数の選び方は
　\\


6.強化学習\\
これはニューラルネットワークの一分野ではないのだが、近年DQN（ヤンキーのことではない）などによりニューラルネットワークを利用した強化学習が進んでおり、また終盤の章の「ニューラルネットのPDE数値解析への応用」はそのタイプの強化学習を用いているので、ここで紹介しておく




探索と行動の二律背反
まるでデュエルマスターズの多色呪文（おそらく水、自然でコスト３くらい。ドローとマナ加速を選べて、色基盤にもなり状況に応じて使えるデッキの潤滑油的な存在なのだろう）のような名前だが、強化学習の用語である。


7.ボルツマンマシン\\
非常に名前がかっこいい機構である。ここまでは決定論的なニューラルネットワークだったが、ここでは確率的ニューラルネットワークについて論じていく。\\


\newpage
\section{ディープラーニング}
いよいよここから、第三次人工知能ブームを巻き起こしたとして巷で話題の深層学習に入り込んでいく。数学的なレベルは前章よりも大幅に上がるので、前章で知識不足を感じた方は関数解析や確率解析,圏論の書籍を片手に読むことを勧める。\\
　\\
深層学習とは\\
前章で扱ったニューラルネットワークは、入力層と隠れ層と出力層が１つずつであったが、ここからは隠れ層が２つ以上のもの、つまり$n$層パーセプトロン$(n>3)$を扱っていく\\
　\\
深層学習の積分表現の課題点\\
積分が入れ子構造になり非常にややこしい\\
 
デノイジング・オートエンコーダ（DAE）とは\\
まず、オートエンコーダについて解説する。ニューラルネットは往々にして「次元削減をしたうえで復元する機構」と言われる。次元削減する機構を「エンコーダ」、次元復元する機構を「デコーダ」という。\\
学習の前に、恒等写像を学習させる機構を「オートエンコーダ」という\\
すなわち、関数$f:x\rightarrow x$を近似する。
デノイジングでは、この入力データにあえてノイズを付与して学習させることで、ノイズに対して頑健なニューラルネットワークを作ることができる。\\
　\\
まずは３層パーセプトロンについて考察する。入力データ$X\in\mathbb{R}^d$は確率密度関数$\pi_0(x)$で定まる確率変数であるとする。ここに$d$次元独立正規分布(分散$t$)の確率変数$\varepsilon$を加える。
\begin{eqnarray}
\tilde{X}:=X+\varepsilon
\end{eqnarray}
そして、2章のモデルに倣い、次の関数を最大化することを考えたい。（2乗誤差最小化）
\begin{eqnarray}
F(g):=-E[|X-g(\tilde{X})|^2]
\end{eqnarray}
中間層の写像$h$と、出力層の写像$k$を用いて、$g=h\circ k$と書ける。この記法は後々深層学習を考えるにあたって非常に重要になる\\
　\\
定理：\\
上記の最適化問題の解は次の輸送写像になる（ジェームスシュタイン推定量？）
\begin{eqnarray}
g(x):=x+t\nabla log[K(x,\tilde{x},t/2)*\pi_0(x)]
\end{eqnarray}
ただし、$K$は熱核で、$K(x,y,t):=(4\pi t)^{-d/2}exp[-|x-y|^2/4t]$となる。つまり、$K(\cdot,\cdot,t/2)$で、時刻$t$のブラウン運動の推移確率密度となる\\
実のところ、この畳み込みは偏微分作用素$e^{t/2\Delta}$の作用と同じである（要検証）\\
この輸送写像は、もっと拡張して楕円型作用素$L$に対して定義できる\\
　\\
定義：\\
$L$を楕円型偏微分作用素。関数$K_t(x,y;L)$を、偏微分方程式$\partial_t u(t,x-y)=Lu(t,x-y)$の解とする。この時、異方性DAEを次のように定義できる
\begin{eqnarray}
\Phi_t(x;L):&=&x+t\nabla log[e^{tL}\pi_0(x)]\nonumber\\
&=&x+t\nabla log[\int_{\mathbb{R}^d}K_t(x,y;L)\pi_0(y)dy]
\end{eqnarray}
これを「ニューラルネットワークの輸送表現」という。つまり、三層パーセプトロンのデノイジングオートエンコーダーによる学習をこの関数で表現できる。\\
　\\
確率密度関数$\pi_0$から生成される確率測度を$P_0$と置く（この確率測度はa.eで一意）。これがルベーグ測度に絶対連続なら、下記の式で定義される$P_t$もすべてルベーグ測度と絶対連続である。
\begin{eqnarray}
P_t:=P_0\circ\Phi_t^{-1}
\end{eqnarray}
ここで、$P_0$が正規分布、$L$がラプラシアンであるとき、連続DAEに対する測度$\mu$（詳細は後述）はWiener測度である。\\
ちなみに、$P_t$のルベーグ測度とのラドンニコディム導関数を$\pi_t$と置くと、次の式が成り立つ
\begin{eqnarray}
\partial_t \pi_t|_{t=0}=-\Delta \pi_0
\end{eqnarray}
　\\
2.深層デノイジング・オートエンコーダーについて\\
いよいよ深層学習を対象として論理展開をしていく。以後、使用する楕円型偏微分作用素はラプラシアンとし、$L$は隠れ層の数を表す整数であるものとする\\
終端時刻は$T$、これを$L$分割し、時間の変分は$\tau_l$（停止時刻とは無関係であることに注意）であらわされるとする。そして、現在時刻を$t_l(:=\tau_0+\tau_1+......+\tau_{l-1})$と表記する。各DAEで加えられるノイズの確率分布は、$\mathcal{N}(0,\tau_lI)$であるとする\\
　\\
そして、$l+1$番目のDAEで計算された写像を$\Phi_l:\mathbb{R}^d\rightarrow\mathbb{R}^d$と表記する\\
ここで、最初の入力データを$X_0$と置くと、これは初記の通り確率密度関数$\pi_0$に従う$\mathbb{R}^d$上の確率変数となる。ここで、確率測度$P_{t_1}:=P_0\circ\Phi_0$を定義し、これと同様に帰納的に$P_{L+1}$まで定義していく\\
また、次のように合成DAEを定義する
\begin{eqnarray}
\Phi^T_{0;L}:=\Phi_L\circ......\circ\Phi_1\circ\Phi_0
\end{eqnarray}
これを用いて、次のように連続DAEが定義できる\\
定義:連続DAE\\
次の連続力学系の解作用素$\phi_t$を連続DAEと呼ぶ
\begin{eqnarray}
dX_t=\nabla log\pi_t[X_t]dt\\
ただし、\pi_tは確率測度P_t:=P_0\circ\phi_t\nonumber
\end{eqnarray}
　\\
極限の存在と一意性は次の定理よりいえる\\ 
　\\
定理：\\
$log \pi_0$は局所リプシッツ連続であるとする。このとき
\begin{eqnarray}
\Phi^T_{0:L}\rightarrow \phi_T ,L\rightarrow \infty
\end{eqnarray}
結局のところ、連続DAEとは、標準ブラウン運動$B_t$を用いて、$x+B_{T-t}$と表記される逆向きブラウン運動に対する、$x$のジェームスシュタイン推定をニュ−ラルネットワークに学習させる機構である。実際の深層DAEはその離散化となる。\\
深層化する意味もここで説明できる。単純にDAEのサンプリングによるドリフト推定するにあたって、観測点を増やしたほうが精度が上がるに決まっている。それと同じことがDAEで起きているのである。\\
　\\
3．積層DAEと合成DAE\\
ここからは、深層DAEについて圏論的・微分幾何学的に考察する

\newpage
\section{ニューラルネットワークの数値解析への応用}
ここでは、強化学習によるBSDE（後退確率微分方程式）の確率制御問題や、それをさらにPDEs（放物型偏微分方程式）の境界値問題への数値計算に応用する手法について解説する。
PDEの数値解析理論は、現状空間が4次元以上だと誤差がひどいことになってしまう。量子の世界に踏み込まない物理学や工学ならそれでもいいかもしれないが、株式の銘柄の数だけ次元が増えるファイナンスや11次元の物理学、その他SDEやPDEの応用分野の多くではこれが大いに問題になる\\
 
今回扱う半線形放物型PDEの境界値問題は次のような形をしている\\
\begin{eqnarray}
\frac{\partial u}{\partial t}(t,x)+\frac{1}{2}(\Delta_xu)(t,x)+f(u(t,x),(\nabla_xu)(t,x))=0\\
u(T,x)=g(x)
\end{eqnarray}
ただし、$x$の取りうる空間は$\mathbb{R}^d$,$f:\mathbb{R}\times\mathbb{R}^d\rightarrow \mathbb{R},g:\mathbb{R}^d\rightarrow\mathbb{R}$は既知の連続関数であるとし、$u:[0,T]\times\mathbb{R}^d\rightarrow \mathbb{R}$は$C^{1,2}$級とする。\\
　\\
非線形ファインマンカッツの公式\\
$W$を$d$次元標準ブラウン運動。確率空間をそこから生成されるものとし、$Y,Z$は適合過程で、$f,g$は先ほどと同じで、$\xi\in\mathbb{R}^d$とする。
\begin{eqnarray}
Y_t=g(t,\xi+W_T)+\int^T_tf(Y_s,Z_s)ds-\int^T_t<Z_t,dW_t>
\end{eqnarray}
は、上記の半線形熱方程式終端値問題の解$u$に対して次を満たす
\begin{eqnarray}
Y_t=u(t,X_t),Z_t=\nabla_xu(t,X_t)
\end{eqnarray}
ただし、$X_t:=\xi+W_t$\\
　\\
このブラウン運動は、学習の反復1回ごとにオイラー丸山法で実装する。\\
今回、このニューラルネットワークを用いた強化学習で学習させる対象は、写像$Z_t=F_t(X_t)$\\
通常の強化学習での評価関数にあたるものは$Y_t=Q(t,Z_t)$と定義される。\\
そしてこの評価関数は、終端時刻$T$にて、
\begin{eqnarray}
\psi(\theta)=||Y_T-g(X_T)||^2
\end{eqnarray}
 で評価され、通常の強化学習と同じく更新されていく。\\
つまり、時刻がN分割の場合、「毎回N回の行動をとった後に一度だけ報酬が与えられる状況での強化学習」と見なして、$X_t$に対する最適戦略$Z_t$と$Y_t$を導き出して、それを偏微分方程式の数値解析結果にするという考えである。\\
通常のQ-学習と違い、状態$X_{t_n}$と方策$Z_{t_n}$と前時刻での評価$Y_{t_{n-1}}$から一意的にこの式で$Y_{t_n}$が定まるため、その意味では楽だといえる。\\
\begin{eqnarray}
Y_{t_{n+1}}\approx Y_{t_n}-f(Y_{t_n},(\nabla_xu)(t_n,\xi+W_{t_n}))(t_{n-2}-t_{n-1})+<(\nabla_xu)(t_n,\xi+W_{t_n}),W_{t_2}-W_{t_1}>
\end{eqnarray}
ニューラルネットは各時刻で用意する。つまりこれはRNNではなく、単純に$N$個のニューラルネットが存在するのみである。時刻が違う中間層同士のつながりはない。\\
損失関数$\psi(\theta)$で計算された結果から、新たなパラメータを決定する。すなわち、学習$m$回目のパラメータを$\Theta_m$とおくと
\begin{eqnarray}
\Theta_{m+1}:=\Theta_{m}-\nabla_\theta\psi(\Theta_m)
\end{eqnarray}
これは要するに確率的勾配法による学習で、ニューラルネットワークに偏微分方程式の正しい関数を近似させようとする取り組みだ。\\
ただし、普通の確率的勾配法はデータの集合$\mathcal{D}$上の一様測度の元で行われていたが、今回は$C([0,T])$上のWiener測度の下で確率的勾配法を行っているのである。\\
　\\
「ならばAdam法などをこの手法に適用してはどうか」「ということは確率過程になるので、抽象Wiener空間上の解析を離散化したものと捉えて収束証明ができるのではないか」「カメロン-マルティン部分空間が絡んでくるのではないか」という当然の疑問が浮かんでくる読者もいるだろうが、それは今後の課題である。

 
 
\newpage
\section{数値解析の深層学習への応用}
今度は逆に、深層学習に対して、微分方程式の数値計算を応用する方法について解説する。\\
　\\ 
　近年、結果をあげている深層学習アルゴリズムの層数は圧倒的に深くなっている。数年前は結果を出しているのはせいぜい８層、22層程度だったが、2015年に某大会で152層のニューラルネットワークが成果を上げたのを皮切りに、現在では1000層を超えるネットワークも使われている。\\
 　\\
 　\\
\scalebox{1.1}{1.残差学習}\\
　なぜそのような超深層学習が可能になったのか、そのカギを握るのは。残差学習という概念だ。\\
　3章を読んでもらえばわかる通り、通常のニューラルネットワークは入力$x$に対して、学習させたい写像$H(x)$を直接学習させる。\\
　しかしこの手法は3～6層程度のパーセプトロンなら有効だが、層の数が劇的に増えてくると意味をなさなくなってくる（勾配消失・発散）。それを突破するためにいろいろな方法が考えられたが、どれも中間層の数が数百、数千層単位になる超深層学習には対応できなかった\\
　そこで登場するのが残差学習である。学習させる対象は、$H(x)$ではなく、残差$F(x):=H(x)-x$、これを学習させ、そこに$x$を加えた値を次の層の入力とする。\\
　ここで、この残差関数$F$の、$n$番目の層での値を$F(t_n,\cdot)$と表記する。また、$n$番目の層の入力を$x_{t_n}$と表記する。\\
　すると、この残差学習はこう書ける。
\begin{eqnarray}
x_{t_{n+1}}=x_{t_n}+F(t_n,x_{t_n})
\end{eqnarray}　
　これまで数学を学んできた読者ならすでにお気づきだろう。\\
　そう。これは常微分方程式のオイラー近似に他ならない。\\
　すなわち、残差学習による超深層学習で行われていることは、常微分方程式の離散近似なのである。\\
　近年の主な超深層学習と常微分方程式数値計算の関係を簡単にまとめると
\begin{eqnarray}
ResNet &-& 前進オイラー法\\
RevNet &-& 後退オイラー法\\
PolyNet &-& 前進オイラー法（連立常微分方程式）\\
FractalNet &-& 2次ルンゲクッタ法
\end{eqnarray}　
さらに[1]の著者は、別の常微分方程式の数値計算技法を使うことで、さらなる計算の高速化及び精度の向上を可能にした。\\
　\\
ここまで書かれたのは、決定論的な超深層学習である。ここにノイズ付加やドロップアウトなどの確率要素を加えると、必然これらの議論は確率微分方程式の話に突入する。








\newpage
参考文献\\
http://www2.itc.kansai-u.ac.jp/~afujioka/2014/ig/141112ig.pdf
https://datumstudio.jp/blog/\\
http://sinhrks.hatenablog.com/entry/2014/12/15/081113
実際の伊藤西尾の定理とはだいぶ形が違う。\\
実際には、「$[0,T]$上のカメロン-マルティン部分空間の正規直交基底列$h_k$と、標準正規分布に従う確率変数列$X_k$に対して、$S_n:=\Sigma^n_{k=0}h_kX_k$から誘導される測度がWiener測度に弱収束する」という定理である。
　\\http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/Bayestheory.html
フィードフォワードニューラルネット　http://ibisforest.org/index.php%E5%A4%9A%E5%B1%A4%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3

残差学習のほうが近似できる関数のクラスが広がるのでは


\end{document}
